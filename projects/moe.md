# ğŸ§  å°è¦æ¨¡LLMã«ãŠã‘ã‚‹ Mixture-of-Expertsï¼ˆMoEï¼‰ã®å®Ÿè£…ã¨æ¤œè¨¼

<div align="center">
  <img src="/assets/images/moe_structure.png" alt="MoEæ§‹é€ ã‚¤ãƒ¡ãƒ¼ã‚¸" width="600">
  <br>
  <strong>å›³1. MoEæ¦‚ç•¥å›³</strong>
</div>

---

## 1. æ¦‚è¦ï¼ˆã‚„ã£ãŸã“ã¨ï¼‰

LLMã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¤§ããå¢—ã‚„ã•ãšã«è¡¨ç¾å®¹é‡ã‚’æ‹¡å¼µã™ã‚‹æ–¹æ³•ã¨ã—ã¦ **Mixture-of-Expertsï¼ˆMoEï¼‰** ã‚’è©¦ã—ã€  
**GPTç³»å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆTinyStoriesï¼‰ã«å¯¾ã—ã¦ã€Œ1å±¤ã®ã¿MoEåŒ–ï¼ˆTop-1ï¼‰ã€** ã—ãŸã¨ãã®åŠ¹æœã‚’æ¤œè¨¼ã—ãŸã€‚  
è©•ä¾¡æŒ‡æ¨™ã¯ä¸»ã« **Validation PPL**ã€ã‚ã‚ã›ã¦ **tokens/sec** ã¨ **ãƒ”ãƒ¼ã‚¯VRAM** ã‚’è¨˜éŒ²ã€‚

---

## 2. MoEã®å®šå¼åŒ–ï¼ˆå®Ÿè£…ã«æ²¿ã£ãŸæœ€å°å½¢ï¼‰

# ğŸ¤– Mixture-of-Experts (MoE)ã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°

ã“ã‚Œã¯ã€MoEãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¨ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°ã«é–¢ã™ã‚‹è¨˜è¿°ã§ã™ã€‚

## 1. ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç¢ºç‡ã®è¨ˆç®—

å„ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚Œè¡¨ç¾ $\mathbf{h} \in \mathbb{R}^d$ã€å°‚é–€å®¶ï¼ˆExpertï¼‰ã®æ•° $E$ã€ãƒ«ãƒ¼ã‚¿è¡Œåˆ— $\mathbf{W}_r \in \mathbb{R}^{E \times d}$ ã‚’ç”¨ã„ã¦ã€ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç¢ºç‡ $\mathbf{p}$ ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚

$$
\mathbf{g} = \mathbf{W}_r \mathbf{h}
$$

$$
\mathbf{p} = \mathrm{softmax}(\mathbf{g})
$$

---

## 2. Top-1 å°‚é–€å®¶ã®é¸æŠ

**Top-1**ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã§ã¯ã€ç¢ºç‡ãŒæœ€å¤§ã®å°‚é–€å®¶ $e^*$ ãŒé¸ã°ã‚Œã€ãã®å°‚é–€å®¶ $f_{e^*}$ ãŒéš ã‚Œè¡¨ç¾ $\mathbf{h}$ ã‚’å‡¦ç†ã—ã¾ã™ã€‚

$$
e^* = \arg\max_e p_e
$$

$$
\mathbf{y} = f_{e^*}(\mathbf{h})
$$

---

## 3. å®¹é‡åˆ¶ç´„ (Capacity Constraint)

å„å°‚é–€å®¶ãŒå‡¦ç†ã§ãã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆå®¹é‡ï¼‰ \(\text{capacity}\) ã¯ãƒãƒƒãƒå†…ãƒˆãƒ¼ã‚¯ãƒ³æ•° \(\text{tokens\_per\_batch}\) ã«åŸºã¥ãã€

$$
\text{capacity}
= \left\lceil
\text{capacity\_factor} \times \frac{\text{tokens\_per\_batch}}{E}
\right\rceil
$$

è¶…éã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã¯ drop ã™ã‚‹ã€‚

---

## 4. è£œåŠ©æå¤±ï¼ˆãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°ï¼‰

åã‚ŠæŠ‘åˆ¶ã®ãŸã‚ã€å°‚é–€å®¶ã®å¹³å‡åˆ©ç”¨ç‡ \(\bar{p}_e\) ã‚’ç”¨ã„ãŸè£œåŠ©æå¤± \(\mathcal{L}_{\text{aux}}\) ã‚’åŠ ãˆã‚‹ã€‚

$$
\mathcal{L}_{\text{aux}}
\propto
E \sum_{e=1}^{E} \bar{p}_e^2
$$

---

## 5. æœ€çµ‚æå¤±é–¢æ•°

æœ€çµ‚æå¤±ã¯ãƒ¡ã‚¤ãƒ³æå¤± \(\mathcal{L}_{\text{CE}}\) ã¨è£œåŠ©æå¤±ã®å’Œï¼š

$$
\mathcal{L} = \mathcal{L}_{\text{CE}} + \lambda\,\mathcal{L}_{\text{aux}},
\qquad \lambda = 0.01
$$

> å®Ÿè£…ã¯ `loss = ce + 0.01 * aux`ã€‚å¿…è¦ã«å¿œã˜ `router_jitter` ã‚’åŠ ç®—ã€‚

## 3. å®Ÿé¨“è¨­å®š

- **ãƒ‡ãƒ¼ã‚¿**: `roneneldan/TinyStories`ï¼ˆtrain/validationï¼‰
- **ãƒ¢ãƒ‡ãƒ«**: GPTç³»ã€\(d_{\text{model}}=512\), \(n_{\text{layer}}=6\), \(n_{\text{head}}=8\), `seq_len=256`
- **MoEåŒ–**: **ç¬¬3å±¤ã®FFNã®ã¿**ã‚’MoEã«ç½®æ›ï¼ˆTop-1ï¼‰
- **æƒãå‡ºã—**: å°‚é–€å®¶æ•° \(E\in\{4,8,16\}\) ã‚’æ¯”è¼ƒ  
  ãƒ«ãƒ¼ã‚¿è¨­å®šã¯ **E=8å›ºå®š**ã§ `capacity_factor âˆˆ {1.0, 1.25, 1.5}` Ã— `router_jitter âˆˆ {0.0, 0.01, 0.05}` ã‚’ã‚°ãƒªãƒƒãƒ‰æ¢ç´¢
- **å­¦ç¿’**: `steps=10,000`, `batch_size=16`, `bf16`, AdamW, plateauç³»æ—©æœŸçµ‚äº†
- **è©•ä¾¡**: Validation **PPL**ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰ã€**tokens/sec**ã€**peak VRAM**
- **å®Ÿè¡Œç’°å¢ƒï¼ˆä¾‹ï¼‰**: NVIDIA GeForce **RTX 5090**ï¼ˆ32GBï¼‰, CUDA **12.8**

---

## 4. çµæœ

### 4.1 å°‚é–€å®¶æ•° \(E\) ã®æ¯”è¼ƒï¼ˆæŠœç²‹ï¼‰

| run                 | best PPL | æ”¹å–„ç‡ vs base |  tok/s | peak mem |
|---------------------|---------:|---------------:|-------:|--------:|
| **baseline_s10000** | **7.084**| â€“              | **158,731** | 4,010MB |
| **moe_e4_s10000**   | 7.023    | **+0.87%**     | 124,978 | 4,106MB |
| **moe_e8_s10000**   | **6.942**| **+2.01%**     | 104,272 | 4,216MB |
| **moe_e16_s10000**  | 6.875    | **+2.96%**     |  78,124 | 4,443MB |

**æ‰€è¦‹**: PPL ã¯ä¸€è²«ã—ã¦æ”¹å–„ã€‚é€Ÿåº¦ã¯å°‚é–€å®¶æ•°ã«å¿œã˜ã¦ä½ä¸‹ã€‚**E=8** ãŒç²¾åº¦ã¨é€Ÿåº¦ã®æŠ˜è¡·ã¨ã—ã¦è‰¯å¥½ã€‚

### 4.2 ãƒ«ãƒ¼ã‚¿è¨­å®šï¼ˆ**E=8**å›ºå®šï¼‰

<div align="center">
  <img src="runs/2025-10-29_16-35-55/router_grid.png" alt="Best PPL Heatmap (E=8)" width="600">
  <br>
  <strong>å›³2. Best PPL Heatmapï¼ˆE=8ï¼‰</strong>
</div>

å¯¾å¿œã™ã‚‹æœ€è‰¯PPLã®æ•°å€¤ï¼š

| RJ \\ CF | 1.0 | 1.25 | **1.5** |
|---------:|----:|-----:|--------:|
| **0.00** | 7.06 | 7.00 | **6.98** |
| **0.01** | 7.04 | 7.00 | **6.99** |
| **0.05** | 7.04 | 7.03 | **6.99** |

**æ‰€è¦‹**: `capacity_factor=1.5` ãŒå¸¸ã«æœ€è‰¯ã€‚`router_jitter` ã®å½±éŸ¿ã¯ä»Šå›ã®æ¡ä»¶ã§ã¯å°ã•ã„ï¼ˆ0ã€œ0.05ã§å·®â‰²0.02ï¼‰ã€‚

---

## 5. ã¾ã¨ã‚ï¼ˆä»Šå›ã®æ¡ä»¶ã§è¨€ãˆã‚‹ã“ã¨ï¼‰

- **1å±¤ã®ã¿ã®Top-1 MoE** ã§ã‚‚ã€**å°è¦æ¨¡ã‚¿ã‚¹ã‚¯ã§PPLãŒç´„1â€“3%æ”¹å–„**ã€‚  
- é€Ÿåº¦ä½ä¸‹ã¨ãƒ¡ãƒ¢ãƒªå¾®å¢—ãŒã‚ã‚‹ãŸã‚ã€**E=8** ãŒå®Ÿç”¨ä¸Šã®ãƒãƒ©ãƒ³ã‚¹ã¨ã—ã¦å¦¥å½“ã€‚  
- ãƒ«ãƒ¼ã‚¿è¨­å®šã§ã¯ **capacity_factor ã®å¯„ä¸ãŒå¤§ããã€1.5 ãŒå®‰å®š**ã€‚`router_jitter` ã¯æœ¬ã‚¿ã‚¹ã‚¯è¦æ¨¡ã§ã¯åŠ¹æœé™å®šçš„ã€‚

> æ¨å¥¨æ—¢å®šå€¤ï¼ˆä»Šå›ã®ç¯„å›²å†…ï¼‰  
> `--moe_num_experts 8  --moe_layer_index 3  --moe_capacity_factor 1.5  --moe_router_jitter 0.0`

---

## 6. å†ç¾ãƒ¡ãƒ¢

- **Baseline**: `--no_moe`ï¼ˆaux=0ï¼‰  
- **MoEæœ‰åŠ¹**: `--moe_num_experts E --moe_layer_index 3`  
  ãƒ«ãƒ¼ã‚¿èª¿æ•´ã¯ `--moe_capacity_factor`, `--moe_router_jitter`  
- ãƒ­ã‚°: `metrics.csv`ï¼ˆ`step, train_loss, aux, val_loss, val_ppl, tokens_per_sec, gpu_mem_mb`ï¼‰  
- ãƒ™ã‚¹ãƒˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ: `pytorch_model.best.bin` ã‚’ä¿å­˜

---

## 7. é™ç•Œã¨ä»Šå¾Œï¼ˆä»Šå›ã®å®Ÿé¨“ã®å¤–å´ã¯è¿°ã¹ãªã„ï¼‰

- ä»Šå›ã¯ **å˜å±¤MoEãƒ»Top-1** ã®æœ€å°æ§‹æˆã«é™å®šã€‚  
- ã•ã‚‰ãªã‚‹æ”¹å–„æ¤œè¨¼ã¯ **è¤‡æ•°å±¤MoE** ã‚„ **Top-2**ã€å°‚é–€å®¶ç‰¹åŒ–ãªã©ã®æ‹¡å¼µå¾Œã«è©•ä¾¡ã™ã‚‹ã€‚

---
