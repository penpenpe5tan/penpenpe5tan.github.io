# ğŸ§  å°è¦æ¨¡LLMã«ãŠã‘ã‚‹ Mixture-of-Expertsï¼ˆMoEï¼‰ã®å®Ÿè£…ã¨æ¤œè¨¼

<div align="center">
  <img src="/assets/images/moe_structure.png" alt="MoEæ§‹é€ ã‚¤ãƒ¡ãƒ¼ã‚¸" width="600">
  <br>
  <strong>å›³1. MoEæ¦‚ç•¥å›³</strong>
</div>

---

# 1. æ¦‚è¦
LLMs ã‚’ **è¨ˆç®—é‡ã‚’å¤§ããå¢—ã‚„ã•ãšã«ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™æ‰‹æ³•** ã¨ã—ã¦ **Mixture-of-Expertsï¼ˆMoEï¼‰** ã‚’æ¤œè¨¼ã™ã‚‹ã€‚

æœ¬ãƒªãƒã‚¸ãƒˆãƒªã§ã¯ å°è¦æ¨¡ GPT ç³»ãƒ¢ãƒ‡ãƒ« ã«å¯¾ã—ã€
**Baseline**ï¼ˆMoE ãªã—ï¼‰ ã¨ **å…¨ãƒ–ãƒ­ãƒƒã‚¯ MoE**ï¼ˆå„ Transformer ãƒ–ãƒ­ãƒƒã‚¯ã® FFN ã‚’ MoE ã«ç½®æ›ãƒ»Top-1 ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼‰ã‚’æ¯”è¼ƒã™ã‚‹ã€‚
è©•ä¾¡ã¯ä¸»ã« **Validation Perplexityï¼ˆPPLï¼‰**ã€ä½µã›ã¦ **tokens/sec** ã¨ **peak VRAM** ã‚’è¨˜éŒ²ã™ã‚‹ã€‚

---

# 2. MoE ã¨ã¯
ä¸€èˆ¬ã«ã€Attention è¨ˆç®—ã¨ FFN ã®é–“ã« **ãƒ«ãƒ¼ã‚¿** ã‚’æŒ¿å…¥ã—ã€FFN ã‚’è¤‡æ•°ã®ã€Œ**å°‚é–€å®¶ï¼ˆexpertsï¼‰**ã€ã¨ã—ã¦ä¸¦åˆ—ã«æŒã¤æ§‹æˆã‚’æŒ‡ã™ã€‚å¤šæ§˜ãªçŸ¥è­˜ã‚’ã‚‚ã¤ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã€æ¨è«–æ™‚ã«å®Ÿéš›ã«å®Ÿè¡Œã•ã‚Œã‚‹ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªï¼‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¢—åŠ ã«æ¯”ã¹ã¦ç›¸å¯¾çš„ã«å°ã•ã„ã€‚

## 2.1 ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼ˆgatingï¼‰
å„ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚Œè¡¨ç¾ ($\mathbf{h}\in\mathbb{R}^d$)ã€å°‚é–€å®¶æ•° ($E$)ã€ãƒ«ãƒ¼ã‚¿è¡Œåˆ— ($\mathbf{W}_r\in\mathbb{R}^{E\times d}$) ã«å¯¾ã—ã€

$$
\begin{aligned}
\mathbf{g} &= \mathbf{W}_r\mathbf{h}, \\
\mathbf{p} &= \text{softmax}(\mathbf{g}).
\end{aligned}
$$

ã“ã“ã§ ($\mathbf{p}\in\mathbb{R}^E$) ã¯å„å°‚é–€å®¶ã‚’é¸ã¶ç¢ºç‡åˆ†å¸ƒã§ã‚ã‚‹ã€‚

## 2.2 Top-1 å°‚é–€å®¶é¸æŠï¼ˆSwitch-styleï¼‰
ç¢ºç‡æœ€å¤§ã®å°‚é–€å®¶ ($e^{*}$) ã®ã¿ã‚’é€šã™ï¼š

$$
\begin{aligned}
e^{*} &= \text{argmax}_{e\in\{1,\dots,E\}} p_e, \\
\mathbf{y} &= f_{e^{*}}(\mathbf{h}).
\end{aligned}
$$

**å‚™è€ƒ**ï¼šSwitch Transformer ã§ã¯å‡ºåŠ›ã‚’ç¢ºç‡ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ãªã„ï¼ˆ$\mathbf{y}=f_{e^{*}}(\mathbf{h})$ï¼‰ã€‚

## 2.3 å®¹é‡åˆ¶ç´„ï¼ˆcapacityï¼‰
ãƒãƒƒãƒå†…ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ ($N_{\text{tok}}$) ã¨ã™ã‚‹ã¨ã€å„å°‚é–€å®¶ã®å‡¦ç†ä¸Šé™ï¼ˆcapacityï¼‰ã¯

$$
\text{capacity}
=\left\lceil
\text{capacity\_factor}\times \frac{N_{\text{tok}}}{E}
\right\rceil.
$$

ä¸Šé™ã‚’è¶…ãˆãŸå‰²ã‚Šå½“ã¦ã¯ dropï¼ˆæ£„å´ï¼‰ã™ã‚‹ã€‚

## 5. ã¾ã¨ã‚ï¼ˆä»Šå›ã®æ¡ä»¶ã§è¨€ãˆã‚‹ã“ã¨ï¼‰

- **1å±¤ã®ã¿ã®Top-1 MoE** ã§ã‚‚ã€**å°è¦æ¨¡ã‚¿ã‚¹ã‚¯ã§PPLãŒç´„1â€“3%æ”¹å–„**ã€‚  
- é€Ÿåº¦ä½ä¸‹ã¨ãƒ¡ãƒ¢ãƒªå¾®å¢—ãŒã‚ã‚‹ãŸã‚ã€**E=8** ãŒå®Ÿç”¨ä¸Šã®ãƒãƒ©ãƒ³ã‚¹ã¨ã—ã¦å¦¥å½“ã€‚  
- ãƒ«ãƒ¼ã‚¿è¨­å®šã§ã¯ **capacity_factor ã®å¯„ä¸ãŒå¤§ããã€1.5 ãŒå®‰å®š**ã€‚`router_jitter` ã¯æœ¬ã‚¿ã‚¹ã‚¯è¦æ¨¡ã§ã¯åŠ¹æœé™å®šçš„ã€‚

> æ¨å¥¨æ—¢å®šå€¤ï¼ˆä»Šå›ã®ç¯„å›²å†…ï¼‰  
> `--moe_num_experts 8  --moe_layer_index 3  --moe_capacity_factor 1.5  --moe_router_jitter 0.0`

---

## 6. å†ç¾ãƒ¡ãƒ¢

- **Baseline**: `--no_moe`ï¼ˆaux=0ï¼‰  
- **MoEæœ‰åŠ¹**: `--moe_num_experts E --moe_layer_index 3`  
  ãƒ«ãƒ¼ã‚¿èª¿æ•´ã¯ `--moe_capacity_factor`, `--moe_router_jitter`  
- ãƒ­ã‚°: `metrics.csv`ï¼ˆ`step, train_loss, aux, val_loss, val_ppl, tokens_per_sec, gpu_mem_mb`ï¼‰  
- ãƒ™ã‚¹ãƒˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ: `pytorch_model.best.bin` ã‚’ä¿å­˜

---

## 7. é™ç•Œã¨ä»Šå¾Œï¼ˆä»Šå›ã®å®Ÿé¨“ã®å¤–å´ã¯è¿°ã¹ãªã„ï¼‰

- ä»Šå›ã¯ **å˜å±¤MoEãƒ»Top-1** ã®æœ€å°æ§‹æˆã«é™å®šã€‚  
- ã•ã‚‰ãªã‚‹æ”¹å–„æ¤œè¨¼ã¯ **è¤‡æ•°å±¤MoE** ã‚„ **Top-2**ã€å°‚é–€å®¶ç‰¹åŒ–ãªã©ã®æ‹¡å¼µå¾Œã«è©•ä¾¡ã™ã‚‹ã€‚

---
