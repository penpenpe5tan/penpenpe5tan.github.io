# ğŸ§  å°è¦æ¨¡LLMã«ãŠã‘ã‚‹ Mixture-of-Expertsï¼ˆMoEï¼‰ã®å®Ÿè£…ã¨æ¤œè¨¼

<div align="center">
  <img src="/assets/images/moe_structure.png" alt="MoEæ§‹é€ ã‚¤ãƒ¡ãƒ¼ã‚¸" width="600">
  <br>
  <strong>å›³1. MoEæ¦‚ç•¥å›³</strong>
</div>

---

## 1. æ¦‚è¦

LLM ã‚’ **è¨ˆç®—é‡ã‚’å¤§ããå¢—ã‚„ã•ãšã«è¡¨ç¾å®¹é‡ã‚’æ‹¡å¼µ**ã™ã‚‹æ‰‹æ³•ã¨ã—ã¦ **Mixture-of-Expertsï¼ˆMoEï¼‰** ã‚’æ¤œè¨¼ã€‚  
æœ¬ãƒªãƒã‚¸ãƒˆãƒªã§ã¯ **GPT ç³»ã®å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆTinyStoriesï¼‰** ã«å¯¾ã—ã€  
**Baselineï¼ˆMoE ãªã—ï¼‰** ã¨ **å…¨å±¤MoEï¼ˆå„ Transformer ãƒ–ãƒ­ãƒƒã‚¯ã® FFN ã‚’ MoE ã«ç½®æ›ãƒ»Top-1 ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼‰** ã‚’æ¯”è¼ƒã™ã‚‹ã€‚

è©•ä¾¡ã¯ä¸»ã« **Validation Perplexityï¼ˆPPLï¼‰**ã€ä½µã›ã¦ **tokens/sec** ã¨ **peak VRAM** ã‚’è¨˜éŒ²ã€‚

---

## 2. MoE ã®å®šå¼åŒ–ï¼ˆå®Ÿè£…æº–æ‹ ã®æœ€å°å½¢ï¼‰

### 2.1 ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç¢ºç‡

å„ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚Œè¡¨ç¾ \( \mathbf{h}\in\mathbb{R}^d \)ã€å°‚é–€å®¶æ•° \( E \)ã€ãƒ«ãƒ¼ã‚¿è¡Œåˆ— \( \mathbf{W}_r\in\mathbb{R}^{E\times d} \) ã«å¯¾ã—ã€

$$
\mathbf{g}=\mathbf{W}_r\mathbf{h},\qquad
\mathbf{p}=\mathrm{softmax}(\mathbf{g})
$$

### 2.2 Top-1 å°‚é–€å®¶é¸æŠï¼ˆSwitch-styleï¼‰

ç¢ºç‡æœ€å¤§ã®å°‚é–€å®¶ \( e^\* \) ã®ã¿ã‚’é€šã™ï¼š

$$
e^\*=\arg\max_e\,p_e,\qquad
\mathbf{y}=f_{e^\*}(\mathbf{h})
$$

### 2.3 å®¹é‡åˆ¶ç´„ï¼ˆCapacityï¼‰

ãƒãƒƒãƒå†…ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ \( \text{tokens\_per\_batch} \) ã¨ã™ã‚‹ã¨ã€å„å°‚é–€å®¶ã®å‡¦ç†ä¸Šé™ã¯

$$
\text{capacity}
=
\left\lceil
\text{capacity\_factor}\times \frac{\text{tokens\_per\_batch}}{E}
\right\rceil
$$

è¶…éåˆ†ã¯ **drop** ã™ã‚‹ã€‚

### 2.4 ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°è£œåŠ©æå¤±ï¼ˆå®Ÿè£…ã©ãŠã‚Šï¼‰

ãƒ«ãƒ¼ã‚¿ã®ã€Œé‡è¦åº¦ã€ \( \mathrm{importance}_e=\mathbb{E}[p_e] \) ã¨å®Ÿå‰²å½“ã¦ã€Œè² è·ã€
\( \mathrm{load}_e=\mathbb{E}[\mathbf{1}\{e^\*=e\}] \) ã‚’ç”¨ã„ã€

$$
\mathcal{L}_{\text{aux}}
=
E\,\sum_{e=1}^{E}
\mathrm{importance}_e\cdot \mathrm{load}_e
$$

> å®Ÿè£…ï¼š`aux_loss = E * (importance * load).sum()`ã€‚

### 2.5 æœ€çµ‚æå¤±

$$
\mathcal{L}=
\mathcal{L}_{\text{CE}}+\lambda\,\mathcal{L}_{\text{aux}},
\qquad
\lambda=0.01
$$

> ã‚³ãƒ¼ãƒ‰ï¼š`loss = ce + 0.01 * aux`ã€‚å¿…è¦ã«å¿œã˜ã¦ãƒ«ãƒ¼ã‚¿ã«å¾®å°ãƒã‚¤ã‚º `router_jitter` ã‚’åŠ ç®—ã€‚

---

## 3. å®Ÿè£…ã¨å®Ÿé¨“æ¡ä»¶

- **MoE é©ç”¨**ï¼š**å…¨å±¤MoE**ï¼ˆã™ã¹ã¦ã® Transformer ãƒ–ãƒ­ãƒƒã‚¯ã§ FFNâ†’MoE ç½®æ›ã€Top-1ï¼‰  
  â€» å…¨å±¤åŒ–ãƒ•ãƒ©ã‚°ï¼š`--moe_all_layers`ï¼ˆ`--moe_layer_index` ã¯ç„¡è¦–ï¼‰
- **ãƒ¢ãƒ‡ãƒ«**ï¼šGPT ç³»  
  \( d_{\text{model}}=512,\ n_{\text{layer}}=6,\ n_{\text{head}}=8,\ \text{seq\_len}=256,\ d_{\text{ff}}=4d \)
- **ãƒ‡ãƒ¼ã‚¿**ï¼š`roneneldan/TinyStories`ï¼ˆ`train` / `validation`ï¼‰
- **ã‚¹ã‚¤ãƒ¼ãƒ—**ï¼š
  - **Experts** \( E\in\{4,8,16\} \)ï¼ˆå…¨å±¤MoEã§å…±é€šï¼‰
  - **Routerï¼ˆE=8 å›ºå®šï¼‰**ï¼š  
    \( \text{capacity\_factor}\in\{1.0,1.25,1.5\} \) Ã— \( \text{router\_jitter}\in\{0.0,0.01,0.05\} \)
- **å­¦ç¿’**ï¼š`steps=10,000`, `batch_size=16`, `bf16(ä»»æ„)`, AdamW, plateau æ—©æœŸçµ‚äº†
- **è©•ä¾¡**ï¼šValidation **PPL**ï¼**tokens/sec**ï¼**peak VRAM**
- **ç’°å¢ƒä¾‹**ï¼šNVIDIA GeForce **RTX 5090**ï¼ˆ32GBï¼‰, CUDA **12.8**
---

## 4. çµæœ

### 4.1 å°‚é–€å®¶æ•° \(E\) ã®æ¯”è¼ƒï¼ˆæŠœç²‹ï¼‰

| run                 | best PPL | æ”¹å–„ç‡ vs base |  tok/s | peak mem |
|---------------------|---------:|---------------:|-------:|--------:|
| **baseline_s10000** | **7.084**| â€“              | **158,731** | 4,010MB |
| **moe_e4_s10000**   | 7.023    | **+0.87%**     | 124,978 | 4,106MB |
| **moe_e8_s10000**   | **6.942**| **+2.01%**     | 104,272 | 4,216MB |
| **moe_e16_s10000**  | 6.875    | **+2.96%**     |  78,124 | 4,443MB |

**æ‰€è¦‹**: PPL ã¯ä¸€è²«ã—ã¦æ”¹å–„ã€‚é€Ÿåº¦ã¯å°‚é–€å®¶æ•°ã«å¿œã˜ã¦ä½ä¸‹ã€‚**E=8** ãŒç²¾åº¦ã¨é€Ÿåº¦ã®æŠ˜è¡·ã¨ã—ã¦è‰¯å¥½ã€‚

### 4.2 ãƒ«ãƒ¼ã‚¿è¨­å®šï¼ˆ**E=8**å›ºå®šï¼‰

<div align="center">
  <img src="runs/2025-10-29_16-35-55/router_grid.png" alt="Best PPL Heatmap (E=8)" width="600">
  <br>
  <strong>å›³2. Best PPL Heatmapï¼ˆE=8ï¼‰</strong>
</div>

å¯¾å¿œã™ã‚‹æœ€è‰¯PPLã®æ•°å€¤ï¼š

| RJ \\ CF | 1.0 | 1.25 | **1.5** |
|---------:|----:|-----:|--------:|
| **0.00** | 7.06 | 7.00 | **6.98** |
| **0.01** | 7.04 | 7.00 | **6.99** |
| **0.05** | 7.04 | 7.03 | **6.99** |

**æ‰€è¦‹**: `capacity_factor=1.5` ãŒå¸¸ã«æœ€è‰¯ã€‚`router_jitter` ã®å½±éŸ¿ã¯ä»Šå›ã®æ¡ä»¶ã§ã¯å°ã•ã„ï¼ˆ0ã€œ0.05ã§å·®â‰²0.02ï¼‰ã€‚

---

## 5. ã¾ã¨ã‚ï¼ˆä»Šå›ã®æ¡ä»¶ã§è¨€ãˆã‚‹ã“ã¨ï¼‰

- **1å±¤ã®ã¿ã®Top-1 MoE** ã§ã‚‚ã€**å°è¦æ¨¡ã‚¿ã‚¹ã‚¯ã§PPLãŒç´„1â€“3%æ”¹å–„**ã€‚  
- é€Ÿåº¦ä½ä¸‹ã¨ãƒ¡ãƒ¢ãƒªå¾®å¢—ãŒã‚ã‚‹ãŸã‚ã€**E=8** ãŒå®Ÿç”¨ä¸Šã®ãƒãƒ©ãƒ³ã‚¹ã¨ã—ã¦å¦¥å½“ã€‚  
- ãƒ«ãƒ¼ã‚¿è¨­å®šã§ã¯ **capacity_factor ã®å¯„ä¸ãŒå¤§ããã€1.5 ãŒå®‰å®š**ã€‚`router_jitter` ã¯æœ¬ã‚¿ã‚¹ã‚¯è¦æ¨¡ã§ã¯åŠ¹æœé™å®šçš„ã€‚

> æ¨å¥¨æ—¢å®šå€¤ï¼ˆä»Šå›ã®ç¯„å›²å†…ï¼‰  
> `--moe_num_experts 8  --moe_layer_index 3  --moe_capacity_factor 1.5  --moe_router_jitter 0.0`

---

## 6. å†ç¾ãƒ¡ãƒ¢

- **Baseline**: `--no_moe`ï¼ˆaux=0ï¼‰  
- **MoEæœ‰åŠ¹**: `--moe_num_experts E --moe_layer_index 3`  
  ãƒ«ãƒ¼ã‚¿èª¿æ•´ã¯ `--moe_capacity_factor`, `--moe_router_jitter`  
- ãƒ­ã‚°: `metrics.csv`ï¼ˆ`step, train_loss, aux, val_loss, val_ppl, tokens_per_sec, gpu_mem_mb`ï¼‰  
- ãƒ™ã‚¹ãƒˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ: `pytorch_model.best.bin` ã‚’ä¿å­˜

---

## 7. é™ç•Œã¨ä»Šå¾Œï¼ˆä»Šå›ã®å®Ÿé¨“ã®å¤–å´ã¯è¿°ã¹ãªã„ï¼‰

- ä»Šå›ã¯ **å˜å±¤MoEãƒ»Top-1** ã®æœ€å°æ§‹æˆã«é™å®šã€‚  
- ã•ã‚‰ãªã‚‹æ”¹å–„æ¤œè¨¼ã¯ **è¤‡æ•°å±¤MoE** ã‚„ **Top-2**ã€å°‚é–€å®¶ç‰¹åŒ–ãªã©ã®æ‹¡å¼µå¾Œã«è©•ä¾¡ã™ã‚‹ã€‚

---
