# ğŸ§  å°è¦æ¨¡LLMã«ãŠã‘ã‚‹ Mixture-of-Expertsï¼ˆMoEï¼‰ã®å®Ÿè£…ã¨æ¤œè¨¼

<div align="center">
  <img src="/assets/images/moe_structure.png" alt="MoEæ§‹é€ ã‚¤ãƒ¡ãƒ¼ã‚¸" width="600">
  <br>
  <strong>å›³1. MoEæ¦‚ç•¥å›³</strong>
</div>

---

# 1. æ¦‚è¦

LLMs ã‚’ **è¨ˆç®—é‡ã‚’å¤§ããå¢—ã‚„ã•ãšã«ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™**æ‰‹æ³•ã¨ã—ã¦ **Mixture-of-Expertsï¼ˆMoEï¼‰** ã‚’æ¤œè¨¼ã™ã‚‹ã€‚
æœ¬ãƒªãƒã‚¸ãƒˆãƒªã§ã¯ **å°è¦æ¨¡ GPT ç³»ãƒ¢ãƒ‡ãƒ«** ã«å¯¾ã—ã€
**Baselineï¼ˆMoE ãªã—ï¼‰** ã¨ **å…¨ãƒ–ãƒ­ãƒƒã‚¯ MoEï¼ˆå„ Transformer ãƒ–ãƒ­ãƒƒã‚¯ã® FFN ã‚’ MoE ã«ç½®æ›ãƒ»Top-1 ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼‰** ã‚’æ¯”è¼ƒã™ã‚‹ã€‚

è©•ä¾¡ã¯ä¸»ã« **Validation Perplexityï¼ˆPPLï¼‰**ã€ä½µã›ã¦ **tokens/sec** ã¨ **peak VRAM** ã‚’è¨˜éŒ²ã™ã‚‹ã€‚

---

# 2. MoE ã¨ã¯

ä¸€èˆ¬ã«ã€Attention è¨ˆç®—ã¨ FFN ã®é–“ã« **ãƒ«ãƒ¼ã‚¿**ã‚’æŒ¿å…¥ã—ã€FFN ã‚’è¤‡æ•°ã®ã€Œå°‚é–€å®¶ï¼ˆexpertsï¼‰ã€ã¨ã—ã¦ä¸¦åˆ—ã«æŒã¤æ§‹æˆã‚’æŒ‡ã™ã€‚å¤šæ§˜ãªçŸ¥è­˜ã‚’ã‚‚ã¤ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã€æ¨è«–æ™‚ã«å®Ÿéš›ã«å®Ÿè¡Œã•ã‚Œã‚‹ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªï¼‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¢—åŠ ã«æ¯”ã¹ã¦ç›¸å¯¾çš„ã«å°ã•ã„ã€‚

## 2.1 ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼ˆgatingï¼‰

å„ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚Œè¡¨ç¾ (\mathbf{h}\in\mathbb{R}^d)ã€å°‚é–€å®¶æ•° (E)ã€ãƒ«ãƒ¼ã‚¿è¡Œåˆ— (\mathbf{W}_r\in\mathbb{R}^{E\times d}) ã«å¯¾ã—ã€

[
\begin{aligned}
\mathbf{g} &= \mathbf{W}_r,\mathbf{h},\
\mathbf{p} &= \operatorname{softmax}(\mathbf{g}).
\end{aligned}
]

ã“ã“ã§ (\mathbf{p}\in\mathbb{R}^E) ã¯å„å°‚é–€å®¶ã‚’é¸ã¶ç¢ºç‡åˆ†å¸ƒã§ã‚ã‚‹ã€‚

## 2.2 Top-1 å°‚é–€å®¶é¸æŠï¼ˆSwitch-styleï¼‰

ç¢ºç‡æœ€å¤§ã®å°‚é–€å®¶ (e^{*}) ã®ã¿ã‚’é€šã™ï¼š

[
\begin{aligned}
e^{*} &= \operatorname*{argmax}*{e\in{1,\dots,E}}; p_e,\
\mathbf{y} &= f*{e^{*}}(\mathbf{h}).
\end{aligned}
]

> å‚™è€ƒï¼šSwitch Transformer ã§ã¯å‡ºåŠ›ã‚’ç¢ºç‡ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ãªã„ï¼ˆ(\mathbf{y}=f_{e^{*}}(\mathbf{h}))ï¼‰ã€‚

## 2.3 å®¹é‡åˆ¶ç´„ï¼ˆcapacityï¼‰

ãƒãƒƒãƒå†…ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ (N_{\text{tok}}) ã¨ã™ã‚‹ã¨ã€å„å°‚é–€å®¶ã®å‡¦ç†ä¸Šé™ï¼ˆcapacityï¼‰ã¯

[
\operatorname{capacity}
=\left\lceil
\operatorname{capacity_factor}\times \frac{N_{\text{tok}}}{E}
\right\rceil.
]

ä¸Šé™ã‚’è¶…ãˆãŸå‰²ã‚Šå½“ã¦ã¯ **drop**ï¼ˆæ£„å´ï¼‰ã™ã‚‹ã€‚

## 2.4 ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°è£œåŠ©æå¤±ï¼ˆå®Ÿè£…ã©ãŠã‚Šï¼‰

ãƒ«ãƒ¼ã‚¿ã®ã€Œé‡è¦åº¦ã€ã‚’ (\operatorname{importance}*e = \mathbb{E}*{\text{batch}}[p_e])ã€å®Ÿå‰²å½“ã¦ã€Œè² è·ã€ã‚’ (\operatorname{load}*e = \mathbb{E}*{\text{batch}}\big[\mathbb{I}(e^{*}=e)\big]) ã¨å®šç¾©ã™ã‚‹ï¼ˆãƒãƒƒãƒå¹³å‡ï¼‰ã€‚

ã“ã®ã¨ãè£œåŠ©æå¤±ã¯

[
\mathcal{L}*{\text{aux}}
= E,\sum*{e=1}^{E} \operatorname{importance}_e\cdot \operatorname{load}_e.
]

> å®Ÿè£…ä¾‹ï¼š`aux_loss = E * (importance * load).sum()`

## 2.5 ãƒ«ãƒ¼ã‚¿ã®å¾®å°ãƒã‚¤ã‚ºï¼ˆjitterï¼‰

å­¦ç¿’ã®å®‰å®šåŒ–ã®ãŸã‚ã€(\mathbf{g}) ã«å¾®å°ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹ã“ã¨ãŒã‚ã‚‹ï¼š

[
\tilde{\mathbf{g}} = \mathbf{g} + \boldsymbol{\epsilon},\qquad \boldsymbol{\epsilon}\sim \mathcal{N}\big(\mathbf{0},\sigma^2\mathbf{I}\big),
]

(\tilde{\mathbf{g}}) ã‚’ç”¨ã„ã¦ softmax ã‚’è¨ˆç®—ã™ã‚‹ã€‚

## 2.6 æœ€çµ‚æå¤±

è¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤± (\mathcal{L}_{\text{CE}}) ã«è£œåŠ©æå¤±ã‚’åŠ ãˆã‚‹ï¼š

[
\mathcal{L} = \mathcal{L}*{\text{CE}} + \lambda,\mathcal{L}*{\text{aux}},
\qquad \lambda = 0.01.
]

> ã‚³ãƒ¼ãƒ‰ä¾‹ï¼š`loss = ce + 0.01 * aux`ã€‚å¿…è¦ã«å¿œã˜ã¦ãƒ«ãƒ¼ã‚¿ã« `router_jitter` ã‚’åŠ ç®—ã™ã‚‹ã€‚

---

# 3. å®Ÿé¨“è¨­è¨ˆï¼ˆè¦ç‚¹ï¼‰

* **ãƒ¢ãƒ‡ãƒ«**ï¼šå°è¦æ¨¡ GPT ç³»ã€Baseline ã¨å…¨ãƒ–ãƒ­ãƒƒã‚¯ MoEï¼ˆFFNâ†’MoEã€Top-1ï¼‰ã€‚
* **è©•ä¾¡**ï¼šValidation PPL ã‚’ä¸»æŒ‡æ¨™ã€ã‚ã‚ã›ã¦ tokens/sec ã¨ peak VRAM ã‚’è¨˜éŒ²ã€‚
* **ãƒã‚¤ãƒ‘ãƒ©**ï¼š`capacity_factor`ã€`router_jitter`ã€(\lambda) ã‚’ãƒ­ã‚°ã«æ®‹ã™ã€‚

---

# 4. å‚è€ƒ

* Shazeer et al., *Switch Transformers*ï¼ˆTop-1 gatingï¼‰ ãªã©ã€‚


## 3. å®Ÿè£…ã¨å®Ÿé¨“æ¡ä»¶

- **MoE é©ç”¨**ï¼š**å…¨å±¤MoE**ï¼ˆã™ã¹ã¦ã® Transformer ãƒ–ãƒ­ãƒƒã‚¯ã§ FFNâ†’MoE ç½®æ›ã€Top-1ï¼‰  
  â€» å…¨å±¤åŒ–ãƒ•ãƒ©ã‚°ï¼š`--moe_all_layers`ï¼ˆ`--moe_layer_index` ã¯ç„¡è¦–ï¼‰
- **ãƒ¢ãƒ‡ãƒ«**ï¼šGPT ç³»  
  \( d_{\text{model}}=512,\ n_{\text{layer}}=6,\ n_{\text{head}}=8,\ \text{seq\_len}=256,\ d_{\text{ff}}=4d \)
- **ãƒ‡ãƒ¼ã‚¿**ï¼š`roneneldan/TinyStories`ï¼ˆ`train` / `validation`ï¼‰
- **ã‚¹ã‚¤ãƒ¼ãƒ—**ï¼š
  - **Experts** \( E\in\{4,8,16\} \)ï¼ˆå…¨å±¤MoEã§å…±é€šï¼‰
  - **Routerï¼ˆE=8 å›ºå®šï¼‰**ï¼š  
    \( \text{capacity\_factor}\in\{1.0,1.25,1.5\} \) Ã— \( \text{router\_jitter}\in\{0.0,0.01,0.05\} \)
- **å­¦ç¿’**ï¼š`steps=10,000`, `batch_size=16`, `bf16(ä»»æ„)`, AdamW, plateau æ—©æœŸçµ‚äº†
- **è©•ä¾¡**ï¼šValidation **PPL**ï¼**tokens/sec**ï¼**peak VRAM**
- **ç’°å¢ƒä¾‹**ï¼šNVIDIA GeForce **RTX 5090**ï¼ˆ32GBï¼‰, CUDA **12.8**
---

## 4. çµæœ

### 4.1 å°‚é–€å®¶æ•° \(E\) ã®æ¯”è¼ƒï¼ˆæŠœç²‹ï¼‰

| run                 | best PPL | æ”¹å–„ç‡ vs base |  tok/s | peak mem |
|---------------------|---------:|---------------:|-------:|--------:|
| **baseline_s10000** | **7.084**| â€“              | **158,731** | 4,010MB |
| **moe_e4_s10000**   | 7.023    | **+0.87%**     | 124,978 | 4,106MB |
| **moe_e8_s10000**   | **6.942**| **+2.01%**     | 104,272 | 4,216MB |
| **moe_e16_s10000**  | 6.875    | **+2.96%**     |  78,124 | 4,443MB |

**æ‰€è¦‹**: PPL ã¯ä¸€è²«ã—ã¦æ”¹å–„ã€‚é€Ÿåº¦ã¯å°‚é–€å®¶æ•°ã«å¿œã˜ã¦ä½ä¸‹ã€‚**E=8** ãŒç²¾åº¦ã¨é€Ÿåº¦ã®æŠ˜è¡·ã¨ã—ã¦è‰¯å¥½ã€‚

### 4.2 ãƒ«ãƒ¼ã‚¿è¨­å®šï¼ˆ**E=8**å›ºå®šï¼‰

<div align="center">
  <img src="runs/2025-10-29_16-35-55/router_grid.png" alt="Best PPL Heatmap (E=8)" width="600">
  <br>
  <strong>å›³2. Best PPL Heatmapï¼ˆE=8ï¼‰</strong>
</div>

å¯¾å¿œã™ã‚‹æœ€è‰¯PPLã®æ•°å€¤ï¼š

| RJ \\ CF | 1.0 | 1.25 | **1.5** |
|---------:|----:|-----:|--------:|
| **0.00** | 7.06 | 7.00 | **6.98** |
| **0.01** | 7.04 | 7.00 | **6.99** |
| **0.05** | 7.04 | 7.03 | **6.99** |

**æ‰€è¦‹**: `capacity_factor=1.5` ãŒå¸¸ã«æœ€è‰¯ã€‚`router_jitter` ã®å½±éŸ¿ã¯ä»Šå›ã®æ¡ä»¶ã§ã¯å°ã•ã„ï¼ˆ0ã€œ0.05ã§å·®â‰²0.02ï¼‰ã€‚

---

## 5. ã¾ã¨ã‚ï¼ˆä»Šå›ã®æ¡ä»¶ã§è¨€ãˆã‚‹ã“ã¨ï¼‰

- **1å±¤ã®ã¿ã®Top-1 MoE** ã§ã‚‚ã€**å°è¦æ¨¡ã‚¿ã‚¹ã‚¯ã§PPLãŒç´„1â€“3%æ”¹å–„**ã€‚  
- é€Ÿåº¦ä½ä¸‹ã¨ãƒ¡ãƒ¢ãƒªå¾®å¢—ãŒã‚ã‚‹ãŸã‚ã€**E=8** ãŒå®Ÿç”¨ä¸Šã®ãƒãƒ©ãƒ³ã‚¹ã¨ã—ã¦å¦¥å½“ã€‚  
- ãƒ«ãƒ¼ã‚¿è¨­å®šã§ã¯ **capacity_factor ã®å¯„ä¸ãŒå¤§ããã€1.5 ãŒå®‰å®š**ã€‚`router_jitter` ã¯æœ¬ã‚¿ã‚¹ã‚¯è¦æ¨¡ã§ã¯åŠ¹æœé™å®šçš„ã€‚

> æ¨å¥¨æ—¢å®šå€¤ï¼ˆä»Šå›ã®ç¯„å›²å†…ï¼‰  
> `--moe_num_experts 8  --moe_layer_index 3  --moe_capacity_factor 1.5  --moe_router_jitter 0.0`

---

## 6. å†ç¾ãƒ¡ãƒ¢

- **Baseline**: `--no_moe`ï¼ˆaux=0ï¼‰  
- **MoEæœ‰åŠ¹**: `--moe_num_experts E --moe_layer_index 3`  
  ãƒ«ãƒ¼ã‚¿èª¿æ•´ã¯ `--moe_capacity_factor`, `--moe_router_jitter`  
- ãƒ­ã‚°: `metrics.csv`ï¼ˆ`step, train_loss, aux, val_loss, val_ppl, tokens_per_sec, gpu_mem_mb`ï¼‰  
- ãƒ™ã‚¹ãƒˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ: `pytorch_model.best.bin` ã‚’ä¿å­˜

---

## 7. é™ç•Œã¨ä»Šå¾Œï¼ˆä»Šå›ã®å®Ÿé¨“ã®å¤–å´ã¯è¿°ã¹ãªã„ï¼‰

- ä»Šå›ã¯ **å˜å±¤MoEãƒ»Top-1** ã®æœ€å°æ§‹æˆã«é™å®šã€‚  
- ã•ã‚‰ãªã‚‹æ”¹å–„æ¤œè¨¼ã¯ **è¤‡æ•°å±¤MoE** ã‚„ **Top-2**ã€å°‚é–€å®¶ç‰¹åŒ–ãªã©ã®æ‹¡å¼µå¾Œã«è©•ä¾¡ã™ã‚‹ã€‚

---
