# 🧠 TinyStoriesMoE: 小規模LLMにおけるMixture-of-Expertsの実装と検証

![MoE構造イメージ](/assets/images/moe_structure.png)
<div align="center">
  **図1. MoE概略図**
</div>
---

## 概要

大規模言語モデル（LLM）は膨大なパラメータを持つが、  
**全てのトークンが全パラメータを使用するわけではない**。  
これを解決するために提案されたのが **Mixture-of-Experts（MoE）構造**である。

MoEでは、Transformerの **FFN層（Feed-Forward Network）を複数の「専門家（Experts）」に分割**し、  
**ルーター（Router）**が各トークンをどの専門家に送るかを動的に決定する。  
これにより「**計算は一部の専門家だけで済むが、全体のパラメータ数は増やせる**」という特徴を持つ。

本研究では、小規模LLM（TinyStoriesモデル）を対象に、  
**Transformerを最小単位でMoE化した場合でも効果が現れるか**を検証した。

---

## 実装と実験設定

- **ベースモデル**: GPT系Transformer（6層, d_model=512, n_head=8）  
- **データセット**: `roneneldan/TinyStories`  
- **学習環境**: A100 GPU × 1、bf16訓練、AdamW最適化、早期停止あり  
- **MoE構成**: 
  - FFNを置換し、`E={4,8,16}` の専門家を設定  
  - ルーターは **Top-1 Routing**、`capacity_factor` と `router_jitter` による調整  
- **損失関数**: 
  \[
  \mathcal{L} = \mathcal{L}_{\text{CE}} + 0.01 \mathcal{L}_{\text{aux}}
  \]
  専門家利用の偏りを抑制する補助項（aux loss）を導入。

---

## 結果

| モード | best PPL | 改善率 vs Base | tok/s | peak VRAM |
|--------|-----------:|---------------:|-------:|-----------:|
| Baseline | **7.084** | – | **158,731** | 4.0GB |
| MoE (E=4) | 7.023 | +0.9% | 124,978 | 4.1GB |
| MoE (E=8) | **6.94** | **+2.0%** | 104,272 | 4.2GB |
| MoE (E=16) | 6.87 | +3.0% | 78,124 | 4.4GB |

- MoE化により一貫してPPLが改善。  
- E=8が **性能と速度の最良バランス**。  
- `capacity_factor=1.5` が最も安定。`router_jitter` の効果は限定的。

---

## 学び・得たスキル

- **Mixture-of-Expertsの基本構造とルーティング制御の理解・実装力**  
- **小規模LLMにおける効率化実験の設計・比較分析スキル**  
- **PyTorchによるカスタム層（Router, Expert）の実装・チューニング能力**  
- **GPU効率や通信・計算コストを考慮した実験運用設計**

---

## 背景と意義

MoEは「モデルを巨大化しながら計算コストを抑える」ための重要技術であり、  
**生成モデルのスケーラビリティと省電力化の両立**に向けたアプローチの一つ。  
今回の実装では、小規模タスクでも効果が確認できたことから、  
**モデル効率化の基礎的な検証環境として有効であることを示した**。

---

## 今後の展望

- **Top-2 Routing**や**複数層MoE化**による拡張  
- **動的Expert入れ替え（Dynamic Routing）**の検証  
- 専門家ごとの文体・タスク特化による挙動分析  
- 分散実行環境での通信コスト最適化

本研究を通して得た知見は、  
「**より軽量で柔軟なLLMの設計**」や「**生成AIの実運用最適化**」につながる。

---

© 2025 penpen5tan
