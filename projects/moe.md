# 🧠 小規模LLMにおける Mixture-of-Experts（MoE）の実装と検証

<div align="center">
  <img src="/assets/images/moe_structure.png" alt="MoE構造イメージ" width="600">
  <br>
  <strong>図1. MoE概略図</strong>
</div>

---

# 1. 概要
LLMs を **計算量を大きく増やさずにモデルパラメータを増やす手法** として **Mixture-of-Experts（MoE）** を検証する。

本リポジトリでは 小規模 GPT 系モデル に対し、
**Baseline**（MoE なし） と **全ブロック MoE**（各 Transformer ブロックの FFN を MoE に置換・Top-1 ルーティング）を比較する。
評価は主に **Validation Perplexity（PPL）**、併せて **tokens/sec** と **peak VRAM** を記録する。

---

# 2. MoE とは
一般に、Attention 計算と FFN の間に **ルータ** を挿入し、FFN を複数の「**専門家（experts）**」として並列に持つ構成を指す。多様な知識をもつモデルを構築でき、推論時に実際に実行される（アクティブな）パラメータは、総パラメータの増加に比べて相対的に小さい。

## 3. 実験設定（データ / モデル / 予算）
- **データ**：`roneneldan/TinyStories`
**ライセンス**：[CDLA Sharing 1.0](https://cdla.dev/sharing-1-0/)
- **モデル（小型 Transformer）**：`d_model=512, n_layer=6, n_head=8, seq_len=256, d_ff=2048`
- **学習**：`batch=16, steps=10,000, AdamW(lr=3e-4), bf16, 早期停止`
- **概算モデルサイズ**（出力ヘッドは埋め込みと非結合, vocab≈50,257※）

- **概算モデルサイズ**（出力ヘッドは埋め込みと非結合, vocab≈50,257※）


| 構成 | 総パラメータ（約） |
|---|---:|
| **Baseline（MoEなし）** | **70.6M** |
| **全層 MoE, E=4** | **108.2M** |
| **全層 MoE, E=8** | **158.6M** |
| **全層 MoE, E=16** | **259.2M** |



## 4. 結果（ハイライト）
### 4.1 詳細テーブル（この5項目のみ）
> `run | final_val_ppl | best_val_ppl | max_tok_s | peak_mem_mb | ppl_improve_vs_base_%`


| run | final_val_ppl | best_val_ppl | max_tok_s | peak_mem_mb | ppl_improve_vs_base_% |
|---|---:|---:|---:|---:|---:|
| all_moe_e16_s10000 | 6.924634 | 6.924634 | 24859.28 | 6609.3 | 2.745611909903262 |
| all_moe_e4_s10000 | 6.959759 | 6.959759 | 64529.91 | 4590.8 | 2.2522919190323174 |
| all_moe_e8_rj0.01_cf1.0_s10000 | 7.166689 | 7.166689 | 68895.34 | 5233.6 | -0.6539772827022005 |
| all_moe_e8_rj0.01_cf1.25_s10000 | 7.01976 | 7.01976 | 68908.1 | 5250.0 | 1.4095960393953764 |
| all_moe_e8_rj0.01_cf1.5_s10000 | 6.951536 | 6.951536 | 68737.0 | 5255.5 | 2.367781464510804 |
| all_moe_e8_rj0.05_cf1.0_s10000 | 7.277579 | 7.277579 | 68952.33 | 5235.3 | -2.2113937606432534 |
| all_moe_e8_rj0.05_cf1.25_s10000 | 6.95818 | 6.95818 | 69277.65 | 5249.5 | 2.274468495988487 |
| **all_moe_e8_rj0.05_cf1.5_s10000** | **6.885307** | **6.885307** | **68356.35** | **5254.2** | **3.297947718614487** |
| all_moe_e8_rj0.0_cf1.0_s10000 | 7.30791 | 7.30791 | 69192.1 | 5232.6 | -2.6373834729025107 |
| all_moe_e8_rj0.0_cf1.25_s10000 | 6.895147 | 6.895147 | 68982.55 | 5246.4 | 3.1597478976843827 |
| all_moe_e8_rj0.0_cf1.5_s10000 | 6.930357 | 6.930357 | 68729.06 | 5249.7 | 2.6652341075472683 |
| **all_moe_e8_s10000** | **6.771634** | **6.771634** | **40400.23** | **5249.1** | **4.894450588999493** |
| **baseline_s10000** | **7.120125** | **7.120125** | **158087.4** | **4009.9** | **0.0** |


### 4.2 可視化（Router グリッドのみ）
> **この図だけ掲載**します：`runs/<timestamp>/router_grid.png`


![Router Grid](runs/<timestamp>/router_grid.png)


**図の読み方**
- **縦軸**：router jitter（RJ ∈ {0.00, 0.01, 0.05}）
- **横軸**：capacity factor（CF ∈ {1.0, 1.25, 1.5}）
- **色**：validation PPL（**暗いほど良い**）


**所見（E=8 固定のグリッド内）**
- 最良セルは **RJ=0.05 × CF=1.5**（`best_val_ppl ≈ 6.885`）。
- ただし **全体最良**はグリッド外の `all_moe_e8_s10000`（CF 指定なし, `best_val_ppl=6.7716`）。
- 実務の初期設定は **RJ=0.00–0.01 × CF=1.25–1.5** が安定。学習が不安定なら RJ を下げ、リソースに余裕があれば CF を上げる。
  
## 5. 考察（何が効いた？より実用的な指針）


### 5.1 精度 vs. コスト（全体像）
- **精度**: 全層 MoE で **E=8** が最良（`best_val_ppl=6.7716`）。E=4 は改善が小さく、E=16 は改善が伸びにくい。
- **速度**: `max_tok_s` は Baseline ≈ **158k** → E=8 ≈ **40k** と低下。**scatter/gather と expert 選別**のオーバーヘッドが要因。
- **メモリ**: **パラメータ数は ~2.25×**（70.6M→158.6M）だが、**学習ピークメモリは ~+31%**（~4.0GB→~5.25GB）。
- 理由: ピークは重み+勾配+最適化器状態+アクティベーションの合算で決まるうえ、**MoE は各トークンで一部の専門家しかアクティブにならない**ため、**総パラメータの増加ほどは同時展開されない**。


### 5.2 ルータ設計（RJ / CF）の効き方
- **RJ（router_jitter）**: 小さく揺らす（0.00–0.01）と **初期偏りを緩和**しやすい。大きすぎる（0.05）と **無駄なランダム化**で PPL 悪化 & 速度低下の傾向。
- **CF（capacity_factor）**: 大きくすると **トークン落ち**が減って精度は安定。ただし **メモリ/速度コスト↑**。本実験では **1.25–1.5**が妥当帯。
- **グリッド所見（E=8 固定）**: 最良セルは **RJ=0.05 × CF=1.5**（`ppl≈6.885`）。ただし**全体最良**はグリッド外の `all_moe_e8_s10000`（`ppl=6.7716`）。


### 5.3 専門家数 E のスケーリング
- **E を増やす**と理論上表現力↑だが、**通信/集約コスト**や **不均衡**で逓減。E=8 が **精度/速度/メモリの均衡点**。
- E=16 は **VRAM ~6.6GB**、速度さらに低下。追加改善は小さい。


### 5.4 実用レシピ（最初に試す設定）
1. **全層 MoE, E=8** を基準。
2. **RJ=0.00–0.01**, **CF=1.25–1.5**。学習が不安定→RJ↓、品質不足→CF↑で微調整。
3. 速度/メモリが厳しいときは **E=4**、あるいは **seq_len / batch を下げる**。`--grad_ckpt` も有効。
4. 速度改善策: 可能なら **カーネル融合**・**バッチ化した gather/scatter** を検討（今後の最適化余地）。


### 5.5 まとめ
- **E=8** と **適度な CF（1.25–1.5）**、**低めの RJ（0–0.01）**が、**精度向上と資源消費のバランス**として最適。
- **MoE は“総パラメータは増やすが、同時アクティブは限定”**という性質により、**ピークメモリの増加は相対的に抑えられる**。
- クリエイティブ応用では **専門家の役割設計**（文体/構成/韻など）でさらなる改善余地あり（→ §6）。


## 6. クリエイティブとの掛け合わせ
- トークン毎に「担当者（専門家）」が変わるため、**文体・表現の多様性**が伸びる。
- **スタイル別専門家**（韻/比喩/構成）など、**役割設計**で創作性を高めやすい。


## 7. 使い方（再現手順）
- ワンコマンド実行：
```bash
bash scripts/run_all.sh # baseline → 全層MoE → グリッド → 集計&可視化
bash scripts/run_all.sh runs/... # 既存 run を再集計

## 5. 考察（何が効いた？）
- **全層 MoE** が安定して PPL を押し下げ、**E=8** がコスパ最良。
- **RJ/CF** は過多だと悪化。まずは **RJ=0.0–0.01 / CF=1.25–1.5** を推奨。
- トレードオフ：E を増やすと **精度↑ / tok/s↓ / VRAM↑**。


## 6. クリエイティブとの掛け合わせ
- トークン毎に「担当者（専門家）」が変わるため、**文体・表現の多様性**が伸びる。
- **スタイル別専門家**（韻/比喩/構成）など、**役割設計**で創作性を高めやすい。


## 7. 使い方（再現手順）
- ワンコマンド実行：
```bash
bash scripts/run_all.sh # baseline → 全層MoE → グリッド → 集計&可視化
bash scripts/run_all.sh runs/... # 既存 run を再集計
