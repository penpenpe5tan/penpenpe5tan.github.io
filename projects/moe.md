# ⚙️ LLM効率化のための Mixture of Experts / 量子化 ミニ実験

![チャート](/assets/images/moe_chart.png)

## 概要
Mixture of Experts (MoE) と 量子化を用いて、
**推論速度・メモリ使用量・精度**のトレードオフを検証しました。

## 実験条件（例）
| 条件 | 簡易精度 | 推論速度 | メモリ |
|------|---------|---------|-------|
| FP16 | 100% | 1.0x | 12GB |
| 8bit量子化 | 98% | 1.4x | 9GB |
| 4bit量子化 | 95% | 1.8x | 7GB |
| MoE(Top-2) | 98% | 1.5x | 8.5GB |

## 使用技術
- Transformers / bitsandbytes / (必要に応じて)Deepspeed  
- LoRA, QLoRA, MoE routing

## コメント
- 4bit量子化で速度↑、メモリ↓。精度低下を5%以内に抑制
- MoEは**精度維持**と**スループット改善**のバランス調整に有効

## 今後の改善
- ベンチマーク項目の拡充（長文生成、QA、複数ドメイン）
- レイテンシ分解（トークナイザ/生成/IO）